# -*- coding: utf-8 -*-
"""UI_JSON.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xll4B1qU3G-JCiClQkqgrnqmeCPnYGam
"""

!pip install objectpath

import pandas as pd
import sklearn
from sklearn.datasets import load_iris
from sklearn import preprocessing as pre
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer
import json 
from collections import defaultdict
from collections import ChainMap
mlp_filename = "/content/ml_process.json"

# creating Initial value
#init value for preprocessing JSON
#init_ppm_dict = [{"p_name":"Null", 
#                         "p_parameter":[{"sample":"Null"}]}]
init_mlp_dict = {"Preprocessing":[],"MLA":[]}
with open(mlp_filename, 'w') as json_file:
  json.dump(init_mlp_dict, json_file, indent=2)

#accessing dict files from JSON
#for Preprocessing Method JSON
newdata={}
with open (mlp_filename) as access_json:
  read_content = json.load(access_json)
  #temp1 = read_content['Preprocessing']
  #temp2 = read_content['MLA']

#for MLA JSON
with open (mlp_filename) as access_mla_json:
  read_mla_content = json.load(access_mla_json)

print (read_content)

#method to parse MLA to JSON
#method for KNN
def knn():
  mla_info = {"name":mla_name,
                "parameter": {"k_value":value}}
  read_content['MLA'].append(mla_info)

  with open(mlp_filename, "w") as f:
      json.dump(read_content, f, indent=2)

#method for RandomForest
def randomForest():

  mla_info = {"name":mla_name,
                "parameter": {"estimator": estimator, 
                                   "max_depth": max_depth}}
  read_content['MLA'].append(mla_info)

  with open(mlp_filename, "w") as f:
      json.dump(read_content, f, indent=2)

#method for AdaBoostRegressor
def adaBoostRegressor():

  mla_info = {"name":mla_name,
                "parameter": {"n_estimator": abr_n_estimator, 
                                   "loss": abr_loss}}
  read_content['MLA'].append(mla_info)

  with open(mlp_filename, "w") as f:
      json.dump(read_content, f, indent=2)

#Parse Preprocessing Method to JSON
def parseDict(s_name, s_parameter_name, s_param):
  preprocessing_info = {"name":s_name, 
                          "parameter":{s_parameter_name: s_param}}
  read_content['Preprocessing'].append(preprocessing_info)
  
  with open(mlp_filename , "w") as f:
      json.dump(read_content, f, indent=2)

#@title Preprocessing Option
pp_StandardScaler = True #@param {type:"boolean"}
si_with_std = "True" #@param ["True", "False"]
pp_Normalizer = True #@param {type:"boolean"}
si_norm = "l1" #@param ["l1", "l2", "max"]
pp_SimpleImputer = True #@param {type:"boolean"}
si_strategy = "most_frequent" #@param ["mean", "median", "most_frequent", "constant"]
pp_MinMaxScaler = True #@param {type:"boolean"}
mms_range = "20" #@param {type:"string"}
pp_Binarization = True #@param {type:"boolean"}
bin_threshold = "1" #@param {type:"string"}
pp_sample = True #@param {type:"boolean"}
sample = "1" #@param {type:"string"}
#@title Feature Selection
pp_VarianceThreshold = True #@param {type:"boolean"}
vt_threshold = "50" #@param {type:"string"}
pp_SelectKBest = True #@param {type:"boolean"}
skb_k_feat = "20" #@param {type:"string"}

if pp_StandardScaler == True:
  name = "StandardScaler"
  param_name = "with_std"
  param_value = si_with_std
  parseDict(name, param_name, param_value)

if pp_Normalizer == True:
  name = "Normalizer"
  param_name = "norm"
  param_value = si_norm
  parseDict(name, param_name, param_value)

if pp_SimpleImputer == True:
  name = "SimpleImputer"
  param_name = "strategy"
  param_value = si_strategy
  parseDict(name, param_name, param_value)

if pp_MinMaxScaler == True:
  name = "MinMaxScaler"
  param_name = "range"
  param_value = mms_range
  parseDict(name, param_name, param_value)

if pp_Binarization == True:
  name = "Binarization"
  param_name = "threshold"
  param_value = bin_threshold
  parseDict(name, param_name, param_value)

if pp_VarianceThreshold == True:
  name = "VarianceThreshold"
  param_name = "threshold"
  param_value = vt_threshold
  parseDict(name, param_name, param_value)

if pp_SelectKBest == True:
  name = "SelectKBest"
  param_name = "k_feat"
  param_value = skb_k_feat
  parseDict(name, param_name, param_value)

#@title Machine Learning Algorithm Option
mla_KNN = True #@param {type:"boolean"}
knn_k_value = "" #@param {type:"string"}
mla_RandomForest = True #@param {type:"boolean"}
rf_n_estimators = "2" #@param {type:"string"}
rf_max_depth = "2" #@param {type:"string"}
mla_AdaBoostRegressor = True #@param {type:"boolean"}
abr_n_estimators = "2" #@param {type:"string"}
abr_loss = "linear" #@param ["linear", "square", "exponential"]

if mla_KNN == True:
  mla_name = "KNN"
  value = knn_k_value
  knn()

if mla_RandomForest == True:
  mla_name = "RandomForest"
  estimator = rf_n_estimators
  max_depth = rf_max_depth
  randomForest()

if mla_AdaBoostRegressor == True:
  mla_name = "AdaBoostRegressor"
  abr_n_estimator = abr_n_estimators
  abr_loss = abr_loss
  adaBoostRegressor()



#version 2
def dict_generator(indict, pre=None):
    pre = pre[:] if pre else []
    if isinstance(indict, dict):
        for key, value in indict.items():
            if isinstance(value, dict):
                for d in dict_generator(value, pre + [key]):
                    yield d
                    print(dict_generator(value, pre + [key] ))
            elif isinstance(value, list) or isinstance(value, tuple):
                for v in value:
                    for d in dict_generator(v, pre + [key]):
                        yield d
            else:
                yield pre + [key, value]
        print(dict_generator(value, pre + [key] ))
    else:
        yield pre + [indict]

def compute():
    with open(mlp_filename)as f:
        dat=f.read()
        json_data=json.loads(dat)
        d=dict(json_data)
        dict_generator(d, None)        
        #print(obj)

if __name__=="__main__":
    compute()

#version 3
with open (mlp_filename) as access_json:
  read_content = json.load(access_json)

def iterate(dictionary): 
    for key, value in dictionary.items(): 
        if isinstance(value, dict): 
            iterate(value) 
            continue 
    print('key {!r} -> value {!r}'.format(key, value)) 
 
iterate(read_content)

#version 4
#for key, value in read_mlp_content.items():
  #print(key, ":", value)
ppm_dict = []
num_of_ppm = len(read_content['Preprocessing'])


for preprocess_method in read_content['Preprocessing']:
  print(preprocess_method['name'])
  ppm_parameter = preprocess_method['parameter']
  print (ppm_parameter)

import json
import objectpath

json_tree = objectpath.Tree(read_mlp_content['Preprocessing']) 
pp_name_tuple = tuple(jsonnn_tree.execute('$..p_name'))
pp_parameter_tuple = tuple(json_tree.execute('$..p_parameter')) 
print (pp_name_tuple)
print (pp_parameter_tuple)

import json
#obj=dict()
mlp_filename = "/content/ml_process.json"


def extract(key,value):
    if(type(value)==str):
        result = "{'"+key+"' : '"+value+"'}"
        print(result)
        #obj[key]=value
        return
        ppe_list [ctr] = result
        print (ppe_list[ctr])
        ctr = ctr + 1
        
    t = 0
    if(type(value)==list):
        for i in value:
            extract(key+"_"+str(t),i)
            t = t + 1
        return
        
    if(type(value)==dict):
        for i in value.keys():
            if key:
                key_i = key+"_"+i
            else:
                key_i = i
            extract(key_i,value[i])
        return
        
def compute():
    with open(mlp_filename)as f:
        dat=f.read()
        json_data=json.loads(dat)
        d=dict(json_data)
        extract(None,d)        
        #print(obj)

if __name__=="__main__":
    compute()